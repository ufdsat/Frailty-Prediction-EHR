{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hcuppy.ccs import CCSEngine\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import auc, make_scorer, roc_curve, plot_roc_curve, plot_precision_recall_curve, classification_report, f1_score, accuracy_score, roc_auc_score, precision_score, recall_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_data = pd.read_csv(\"./../data/pat_data_new.csv\")\n",
    "ccs_data = pd.read_csv(\"./../data/ccs_data_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'mmse_3word_repeat', 'mmse_3word_recall'\n",
    "pat_data.drop(['mmse_3word_repeat', 'mmse_3word_recall', 'Charlson_Comorbidity_Index', 'complication_sum'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_dict = {}\n",
    "file = open(\"./../data/ccs.txt\")\n",
    "for line in file.readlines():\n",
    "    strs = line.split(\" \")\n",
    "    ccs = 'ccs_' + strs[0]\n",
    "    desc = \" \".join(strs[1:]).strip()\n",
    "    ccs_dict[ccs] = desc\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ccs_dict[x] for x in ccs_data.columns[1:]]\n",
    "ccs_data.columns = ['studyid'] + cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Services = np.unique(pat_data.Service.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_by_service(basic_data, ccs_data, service_name):\n",
    "    # step 1, preprocess basic data \n",
    "    pdata = basic_data.copy()\n",
    "    pats = basic_data.loc[basic_data.Service.isin(service_name), \"studyid\"]\n",
    "    pdata = pdata.loc[pdata.Service.isin(service_name)]\n",
    "    # step 1.1, remove unrelevant variables\n",
    "    # step 1.2, label the categorical variables\n",
    "    \n",
    "    print(pdata.columns)\n",
    "    # step1, extract diagnosis data\n",
    "    \n",
    "    diag_data = ccs_data[ccs_data.studyid.isin(pats)]\n",
    "    print(\"____\", diag_data.shape, '----')\n",
    "    \n",
    "    # rank top diag_data\n",
    "    names = diag_data.columns[1:]\n",
    "    counts = np.sum(diag_data.values[:, 1:], axis=0)\n",
    "    \n",
    "    print(\"There are %d patients in the selected services\"%diag_data.shape[0])\n",
    "    ccs_codes = names[(np.sum(diag_data.values[:, 1:], axis=0)/diag_data.shape[0] > 0.1)]\n",
    "    ccs_counts = counts[(np.sum(diag_data.values[:, 1:], axis=0)/diag_data.shape[0] > 0.1)]\n",
    "    \n",
    "    ccs_rank = pd.DataFrame({\"ccs_codes\" : ccs_codes,\n",
    "                             \"ccs_counts\" : ccs_counts})\n",
    "    \n",
    "    ccs_rank.sort_values(\"ccs_counts\", ascending=False, inplace=True)\n",
    "    ccs_rank.index = range(ccs_rank.shape[0])\n",
    "    \n",
    "    # extract top diagnosis (ccs) data\n",
    "    top_ccs_codes = ccs_rank.ccs_codes.values.tolist()\n",
    "    top_ccs_codes.append(\"studyid\")\n",
    "    top_ccs_codes_dat = diag_data[top_ccs_codes]\n",
    "    \n",
    "    #print(\"The top\", len(ccs_codes),  \"CCS groups are used in this service, where all of them appearred in at least 10% of the patients.\")\n",
    "    #print(\"CCS groups used in this service are: \", top_ccs_codes[:-1])\n",
    "    #print(\"The rank of the CCS groups are shown in below:\")\n",
    "    #print(ccs_rank)\n",
    "    \n",
    "    # step 3 combine them together\n",
    "    ret_val = pdata.merge(top_ccs_codes_dat, how=\"left\", on=\"studyid\")\n",
    "    \n",
    "    return(ret_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myscoring = {'bal_acc': 'balanced_accuracy',\n",
    "             'roc_auc': 'roc_auc',\n",
    "             'ave_pre': 'average_precision',\n",
    "             'sensitivity': 'recall',\n",
    "             'f1_score' : 'f1',\n",
    "             'precision' : 'precision',\n",
    "             'specificity': make_scorer(recall_score,pos_label=0)\n",
    "             \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'clf__learning_rate': [0.01, 0.05, 0.1],  #3\n",
    "        'clf__max_depth': [3, 5, 7, 10], # 4\n",
    "        'clf__min_child_weight': [1, 3, 5], # 3\n",
    "        'clf__subsample': [0.5, 0.7], # 2\n",
    "        'clf__colsample_bytree': [0.5, 0.7], # 2\n",
    "        'clf__n_estimators' : [100, 200], # 2\n",
    "        'clf__objective': ['binary:logistic']\n",
    "        }\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.style.use(plt.rcParamsDefault)\n",
    "#plt.rcParams['axes.facecolor'] = 'black'\n",
    "def xgboost(data, title):\n",
    "    y = data[\"Frailty\"]\n",
    "    X = data.drop(columns = [\"Frailty\", \"Service\", \"studyid\"])\n",
    "    \n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n",
    "    \n",
    "    ratio = np.round((len(y)-sum(y))/sum(y), 1)\n",
    "        \n",
    "    # categorical data\n",
    "    \n",
    "    numerical_features = ['age', 'HEMATOCRIT', 'HEMOGLOBIN', 'PLATELET_COUNT']\n",
    "    \n",
    "    categorical_impute_features = ['sex', 'Edu_Years']\n",
    "    \n",
    "    categorical_encode_features = ['Race', 'Marital_Status', 'ASA_Anest_Record', 'Patient_Type',\n",
    "                                   'EmployeeStatus']\n",
    "    \n",
    "    \n",
    "    numerical_transformer = Pipeline(\n",
    "        steps = [(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "                 (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "    categorical_imputer = Pipeline(\n",
    "        steps = [(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "                 (\"encoder\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan))]\n",
    "    )\n",
    "    \n",
    "    categorical_encoder = OrdinalEncoder()\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_transformer, numerical_features),\n",
    "            (\"cat_imputer\", categorical_imputer, categorical_impute_features),\n",
    "            (\"cat_encoder\", categorical_encoder, categorical_encode_features),\n",
    "        ], remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "        \n",
    "    model = XGBClassifier(random_state = 24, scale_pos_weight=ratio)\n",
    "    \n",
    "    pipe = Pipeline(\n",
    "        steps=[(\"preprocessor\", preprocessor),\n",
    "               (\"clf\", model)]\n",
    "    )\n",
    "    \n",
    "    clf = GridSearchCV(pipe, params, scoring=\"roc_auc\", n_jobs=-1,\n",
    "                       cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=24),\n",
    "                       verbose=1, refit=True)\n",
    "    \n",
    "    nested_score = cross_validate(clf, X=X, y=y, cv=outer_cv, return_train_score=True, \n",
    "                                  return_estimator=True, scoring=myscoring)\n",
    "\n",
    "    ############ SHAP VALUES COMPUTATION FOR EACH FOLD ##############\n",
    "    iter_shap = 0\n",
    "    preprocessor.fit_transform(X)\n",
    "    feature_names = preprocessor.transformers_[0][2] + preprocessor.transformers_[1][2] + preprocessor.transformers_[2][2] + X.columns[preprocessor.transformers_[3][2]].tolist()\n",
    "                \n",
    "    aucs_ = []\n",
    "    confusion_matrices_ = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    fig, ax=plt.subplots(figsize=(8,8))\n",
    "\n",
    "    for train_index, test_index in outer_cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train = pd.DataFrame(X_train, columns = feature_names)\n",
    "        X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "        \n",
    "        pickle.dump(train_index, open(\"./../result/result-XGBoost/Splited Index/train_idx_\"+str(iter_shap)+\".pkl\", 'wb'))\n",
    "        clf_loaded = nested_score['estimator'][iter_shap]\n",
    "        # WAHT TO DO\n",
    "        # SAVE THIS MODEL\n",
    "        pickle.dump(clf_loaded, open(\"./../result/result-XGBoost/Trained Models/trained_models_\"+str(iter_shap)+\".pkl\", 'wb'))\n",
    "        \n",
    "        X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "        X_test_transformed = preprocessor.transform(X_test)\n",
    "        \n",
    "        X_train_transformed = pd.DataFrame(X_train_transformed, columns=preprocessor.transformers_[0][2] + \n",
    "                                                preprocessor.transformers_[1][2] + \n",
    "                                                preprocessor.transformers_[2][2] + \n",
    "                                                X.columns[preprocessor.transformers_[3][2]].tolist())\n",
    "        \n",
    "        X_test_transformed = pd.DataFrame(X_test_transformed, columns=preprocessor.transformers_[0][2] + \n",
    "                                                preprocessor.transformers_[1][2] + \n",
    "                                                preprocessor.transformers_[2][2] + \n",
    "                                                X.columns[preprocessor.transformers_[3][2]].tolist())\n",
    "        X_train_transformed.index = train_index\n",
    "        X_test_transformed.index = test_index\n",
    "        \n",
    "        pickle.dump(X_train_transformed, open(\"./../result/result-XGBoost/Splited Data/training_set_\"+str(iter_shap)+\".pkl\", 'wb'))\n",
    "        pickle.dump(X_test_transformed, open(\"./../result/result-XGBoost/Splited Data/test_set_\"+str(iter_shap)+\".pkl\", 'wb'))\n",
    "        ########## Calculate more performance metrics #########\n",
    "        y_pred = clf_loaded.predict(X_test)\n",
    "        y_pred_score = clf_loaded.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        _fpr, _tpr, _thresholds = roc_curve(y_pred, y_pred_score, pos_label=1)\n",
    "    \n",
    "        viz = plot_roc_curve(nested_score['estimator'][iter_shap].best_estimator_, X_test, y_test,\n",
    "                         name='ROC fold {}'.format(iter_shap+1),\n",
    "                         alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr=np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "    \n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_score)\n",
    "        auc_ = auc(fpr, tpr)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        aucs_.append(auc_)\n",
    "        confusion_matrices_.append(cm)\n",
    "        ############## Save the predicted result#################\n",
    "        y_pred_df = pd.Series(y_pred)\n",
    "        y_pred_df.index = test_index\n",
    "        pickle.dump(y_pred_df, open(\"./../result/result-XGBoost/Predict Result/\"+\"testfold\"+str(iter_shap)+'.pkl', 'wb'))\n",
    "        \n",
    "        y_pred_score_df = pd.Series(y_pred_score)\n",
    "        y_pred_score_df.index = test_index\n",
    "        pickle.dump(y_pred_score_df, open(\"./../result/result-XGBoost/Predict Probability/\"+\"testfold\"+str(iter_shap)+'.pkl', 'wb'))\n",
    "        \n",
    "        # on training data\n",
    "        train_tmp_df = pd.DataFrame(preprocessor.fit_transform(X_train), columns=feature_names)\n",
    "        train_explainer = shap.TreeExplainer(clf_loaded.best_estimator_[\"clf\"])\n",
    "        \n",
    "        train_shap_values = pd.DataFrame(train_explainer.shap_values(train_tmp_df), columns=feature_names)\n",
    "        train_shap_values.index = train_index\n",
    "        \n",
    "        # on test data\n",
    "        test_tmp_df = pd.DataFrame(preprocessor.fit(X_train).transform(X_test), columns=feature_names)\n",
    "        test_explainer = shap.TreeExplainer(clf_loaded.best_estimator_[\"clf\"])\n",
    "        \n",
    "        test_shap_values = pd.DataFrame(test_explainer.shap_values(test_tmp_df), columns=feature_names)\n",
    "        test_shap_values.index = test_index\n",
    "        \n",
    "        #print(\"./../result/SHAP Values/\"+\"train_\"+str(i)+\"_fold_\"+str(iter_shap)+'.pkl')\n",
    "        pickle.dump(train_shap_values, open(\"./../result/result-XGBoost/SHAP Values/\"+\"trainfold_\"+str(iter_shap)+'.pkl', 'wb'))\n",
    "        pickle.dump(test_shap_values, open(\"./../result/result-XGBoost/SHAP Values/\"+\"testfold_\"+str(iter_shap)+'.pkl', 'wb'))\n",
    "        \n",
    "        iter_shap += 1\n",
    "        print(iter_shap)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "    return nested_score, aucs_, confusion_matrices_\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_score, auc_, confusion_matrix_ =xgboost(create_data_by_service(pat_data, ccs_data, Services), \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_array = []\n",
    "for matrix in confusion_matrix_:\n",
    "    acc = (matrix[0, 0] + matrix[1, 1])/np.sum(matrix)\n",
    "    acc_array.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(acc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc_test_scores = np.round(np.mean(nested_score['test_bal_acc']), 2)\n",
    "roc_auc_test_scores = np.round(np.mean(nested_score['test_roc_auc']), 2)\n",
    "ave_pre_test_scores = np.round(np.mean(nested_score['test_ave_pre']), 2)\n",
    "recall_test_scores = np.round(np.mean(nested_score['test_sensitivity']), 2)\n",
    "f1_test_scores = np.round(np.mean(nested_score['test_f1_score']), 2)\n",
    "precision_test_scores = np.round(np.mean(nested_score['test_precision']), 2)\n",
    "specificity_test_scores = np.round(np.mean(nested_score['test_specificity']), 2)\n",
    "    \n",
    "bal_acc_test_std = np.round(np.std(nested_score['test_bal_acc']), 2)\n",
    "roc_auc_test_std = np.round(np.std(nested_score['test_roc_auc']), 2)\n",
    "ave_pre_test_std = np.round(np.std(nested_score['test_ave_pre']), 2)\n",
    "recall_test_std = np.round(np.std(nested_score['test_sensitivity']), 2)\n",
    "f1_test_std = np.round(np.std(nested_score['test_f1_score']), 2)\n",
    "precision_test_std = np.round(np.std(nested_score['test_precision']), 2)\n",
    "specificity_test_std = np.round(np.std(nested_score['test_specificity']), 2)\n",
    "\n",
    "print('bal_acc: ' + str(bal_acc_test_scores) + ' SD: ' + str(bal_acc_test_std))\n",
    "print('roc_auc: ' + str(roc_auc_test_scores) + ' SD: ' + str(roc_auc_test_std))\n",
    "print('ave_pre: ' + str(ave_pre_test_scores)  + ' SD: ' + str(ave_pre_test_std))\n",
    "print('recall: ' + str(recall_test_scores) + ' SD: ' + str(recall_test_std))\n",
    "print('f1_score: ' + str(f1_test_scores) + ' SD: ' + str(f1_test_std))\n",
    "print('precision: ' + str(precision_test_scores) + ' SD: ' + str(precision_test_std))\n",
    "print('specificity: ' + str(specificity_test_scores) + ' SD: ' + str(specificity_test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc_train_scores = np.mean(nested_score['train_bal_acc'])\n",
    "roc_auc_train_scores = np.mean(nested_score['train_roc_auc'])\n",
    "ave_pre_train_scores = np.mean(nested_score['train_ave_pre'])\n",
    "recall_train_scores = np.mean(nested_score['train_sensitivity'])\n",
    "f1_train_scores = np.mean(nested_score['train_f1_score'])\n",
    "precision_train_scores = np.mean(nested_score['train_precision'])\n",
    "specificity_trian_scores = np.mean(nested_score['train_specificity'])\n",
    "\n",
    "bal_acc_test_scores = np.mean(nested_score['test_bal_acc'])\n",
    "roc_auc_test_scores = np.mean(nested_score['test_roc_auc'])\n",
    "ave_pre_test_scores = np.mean(nested_score['test_ave_pre'])\n",
    "recall_test_scores = np.mean(nested_score['test_sensitivity'])\n",
    "f1_test_scores = np.mean(nested_score['test_f1_score'])\n",
    "precision_test_scores = np.mean(nested_score['test_precision'])\n",
    "specificity_test_scores = np.mean(nested_score['test_specificity'])\n",
    "\n",
    "print('bal_acc ' + str(bal_acc_test_scores))\n",
    "print('roc_auc ' + str(roc_auc_test_scores))\n",
    "print('ave_pre ' + str(ave_pre_test_scores))\n",
    "print('recall ' + str(recall_test_scores))\n",
    "print('f1_score ' + str(f1_test_scores))\n",
    "print('specificity ' + str(specificity_test_scores))\n",
    "print('precision ' + str(precision_test_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML+Pytorch)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
